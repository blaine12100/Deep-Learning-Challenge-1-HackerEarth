{
    "cells": [
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": [], 
            "source": "from PIL import Image\nimport numpy as np\nimport glob\nfrom numpy import array\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport tensorflow as tf\nfrom io import BytesIO  \nimport requests  \nimport json\n\ndef loading_saving_image_as_grayscale_train(img):\n    loading=Image.open(img)\n    \n    loading=loading.resize((28,28),Image.ANTIALIAS)\n\n    loading=loading.convert('L')\n\n    conversion_to_array=np.asarray(loading,dtype=float)\n\n    train_data.append(conversion_to_array)\n\ndef loading_saving_image_as_grayscale_test(img):\n    loading=Image.open(img,'r')\n    \n    loading=loading.resize((28,28),Image.ANTIALIAS)\n\n    loading=loading.convert('L')\n    \n\n    conversion_to_array=np.asarray(loading,dtype=float)\n\n    test_data.append(conversion_to_array)\n\n\n\ntest_data=[]\ntrain_data=[]\n\ntrain=glob.glob('train_img/*.png')\ntest=glob.glob('test_img/*.png')\n\nfor data in train:\n    loading_saving_image_as_grayscale_train(data)\n\nfor item in test:\n    loading_saving_image_as_grayscale_test(item)\n\ndef OneHot(label,n_classes):\n    label=np.array(label).reshape(-1)\n    label=np.eye(n_classes)[label]\n\n    return label\n\ndataframe=pd.read_csv('train.csv')\ntrain_data=np.asarray(train_data)\ntest_data=np.asarray(test_data)\nuni=dataframe['label']\n\ndataframe1=pd.read_csv('test.csv')\ndataframe1.index=dataframe1.index+1\nonly_index=dataframe1['image_id']\n\n\nlabel=LabelEncoder()\ninteger_encoding=label.fit_transform(uni)\n\n\nbinary=OneHotEncoder(sparse=False)\ninteger_encoding=integer_encoding.reshape(len(integer_encoding),1)\nonehot=binary.fit_transform(integer_encoding)\n\ntrain_data=np.reshape(train_data,[-1,28,28,1])\ntest_data=np.reshape(test_data,[-1,28,28,1])\n\ntrain_data=np.transpose(train_data,(0,2,1,3))\ntest_data=np.transpose(test_data,(0,2,1,3))\n\ntrain_data=train_data.astype(np.float32)\ntest_data=test_data.astype(np.int32)\n\n'''HyperParameters\nBatch size=10\nDecay rate10000,0.02\nImage size=28\nNo of Iterations-300000\nSize of Network-8 layers\nOptimizer-SGD'''\nbatch_size = 50\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    # placeholders for input data batch_size x 32 x 32 x 3 and labels batch_size x 10\n    data_placeholder = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])\n    label_placeholder = tf.placeholder(tf.int32, shape=[batch_size, 25])\n    data_placeholder2 = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])\n\n    tf_test_dataset = tf.placeholder(tf.float32, shape=(len(test_data), 28, 28, 1))\n\n    # defining decaying learning rate\n    global_step = tf.Variable(0)\n    decay_rate = tf.train.exponential_decay(1e-6, global_step=global_step, decay_steps=10000, decay_rate=0.02)\n    \n    layerCNN01_weights= tf.Variable(tf.truncated_normal([5,5, 1,64], stddev=0.1))\n    layerCNN01_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    \n    layerCNN02_weights= tf.Variable(tf.truncated_normal([5,5,64,64], stddev=0.1))\n    layerCNN02_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    \n    layerCNN03_weights= tf.Variable(tf.truncated_normal([5, 5,64,128], stddev=0.1))\n    layerCNN03_biases = tf.Variable(tf.constant(0.1, shape=[128]))\n    \n    layer0_weights= tf.Variable(tf.truncated_normal([4,4,128,128], stddev=0.1))\n    layer0_biases = tf.Variable(tf.constant(0.1, shape=[128]))\n\n    layer1_weights = tf.Variable(tf.truncated_normal([3,3,128,256], stddev=0.1))\n    layer1_biases = tf.Variable(tf.constant(0.1, shape=[256]))\n\n    layer2_weights = tf.Variable(tf.truncated_normal([3,3,256,256], stddev=0.1))\n    layer2_biases = tf.Variable(tf.constant(0.1, shape=[256]))\n\n    layer3_weights = tf.Variable(tf.truncated_normal([2,2,256,512], stddev=0.1))\n    layer3_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n\n    layer4_weights = tf.Variable(tf.truncated_normal([512, 25], stddev=0.1))\n    layer4_biases = tf.Variable(tf.constant(0.1, shape=[25]))\n\n    layer5_weights = tf.Variable(tf.truncated_normal([25, 25], stddev=0.1))\n    layer5_biases = tf.Variable(tf.constant(0.1, shape=[25]))\n\n\n    def layer_multiplication(data_input_given,dropping=False):\n        \n        CNN01=tf.nn.relu(\n            tf.nn.conv2d(data_input_given, layerCNN01_weights, strides=[1, 1, 1, 1], padding='SAME') + layerCNN01_biases)\n        \n        CNN02=tf.nn.relu(\n            tf.nn.conv2d(CNN01, layerCNN02_weights, strides=[1, 1, 1, 1], padding='SAME') + layerCNN02_biases)\n            \n        CNN03=tf.nn.relu(\n            tf.nn.conv2d(CNN02, layerCNN03_weights, strides=[1, 1, 1, 1], padding='SAME') + layerCNN03_biases)\n        \n        CNN04 = tf.nn.relu(\n            tf.nn.conv2d(CNN03, layer0_weights, strides=[1, 1, 1, 1], padding='SAME') + layer0_biases)\n        \n        Pool04=tf.nn.max_pool(CNN04, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        \n\n        CNN1 = tf.nn.relu(\n            tf.nn.conv2d(Pool04, layer1_weights, strides=[1, 1, 1, 1], padding='SAME') + layer1_biases)\n        \n        print(\"CNN0 Done\")\n\n        # Pooling Layer\n\n        Pool1 = tf.nn.max_pool(CNN1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        print('Pool1 DOne')\n        print(Pool1.shape)\n\n        # second Convolution layer\n\n        CNN2 = tf.nn.relu(tf.nn.conv2d(Pool1, layer2_weights, strides=[1, 1, 1, 1], padding='SAME')) + layer2_biases\n        print('CNN2 Done')\n        print(CNN2.shape)\n        # Second Pooling\n\n        Pool2 = tf.nn.max_pool(CNN2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        print('pool2 Done')\n        print(Pool2.shape)\n        # Third Convolutional Layer\n\n        CNN3 = tf.nn.relu(tf.nn.conv2d(Pool2, layer3_weights, strides=[1, 1, 1, 1], padding='SAME')) + layer3_biases\n        print('CNN3 Done')\n        print(CNN3.shape)\n        # Third Pooling Layer\n\n        Pool3 = tf.nn.max_pool(CNN3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n        print('Pool3 DOne')\n        print(Pool3.shape)\n        # Fully Connected Layer\n        Pool4 = tf.nn.max_pool(Pool3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n        print(Pool4.shape)\n\n        shape = Pool4.get_shape().as_list()\n\n        # print(shape)\n\n        reshape = tf.reshape(Pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n\n        print(reshape.shape)\n\n        FullyCon = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n\n        print(FullyCon.shape)\n        \n        if dropping==False:\n            print(\"Training\")\n            dropout = tf.nn.dropout(FullyCon, 0.6)\n            final_logit = tf.matmul(dropout, layer5_weights) + layer5_biases\n            print(final_logit.shape)\n            first_index = final_logit[0]\n            print(first_index)\n            return final_logit\n        \n        else:\n            print(\"Testing\")\n            final_logit = tf.matmul(FullyCon, layer5_weights) + layer5_biases\n            print(final_logit.shape)\n            first_index = final_logit[0]\n            print(first_index)\n            return final_logit\n\n\n    train_input = layer_multiplication(data_placeholder)\n    test_prediction = tf.nn.softmax(layer_multiplication(tf_test_dataset,True))\n    print(train_input.shape)\n    print(test_prediction.shape)\n\n    loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label_placeholder, logits=train_input))\n            + 0.01 * tf.nn.l2_loss(layer0_weights)\n            + 0.01 * tf.nn.l2_loss(layer1_weights) \n            + 0.01 * tf.nn.l2_loss(layer2_weights)\n            + 0.01 * tf.nn.l2_loss(layer3_weights)\n            + 0.01 * tf.nn.l2_loss(layer4_weights)\n            + 0.01 * tf.nn.l2_loss(layer5_weights)\n            + 0.01 * tf.nn.l2_loss(layerCNN01_weights)\n            + 0.01 * tf.nn.l2_loss(layerCNN02_weights)\n            + 0.01 * tf.nn.l2_loss(layerCNN03_weights)\n            )\n\n    optimizer = tf.train.GradientDescentOptimizer(name='Stochastic', learning_rate=decay_rate).minimize(loss,\n                                                                                                        global_step=global_step)\n        \n    #optimizer=tf.train.AdamOptimizer(learning_rate=decay_rate).minimize(loss,global_step=global_step)\n    \n    #optimizer=tf.train.RMSPropOptimizer(learning_rate=decay_rate,decay=0.9).minimize(loss,global_step=global_step)\n\n    num_steps = 300000\n\n    prediction = []\n\n    cwd=os.getcwd()\n    saver=tf.train.Saver()\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        for i in range(num_steps):\n    \n            offset = (i * batch_size) % (onehot.shape[0] - batch_size)\n            batch_data = train_data[offset:(offset + batch_size), :, :]\n            batch_labels = onehot[offset:(offset + batch_size), :]\n            \n            if(i%500==0):\n                \n                feed_dict = {data_placeholder: batch_data, label_placeholder: batch_labels}\n                _, l, predictions = session.run(\n                    [optimizer, loss, train_input], feed_dict=feed_dict)\n                print('Minibatch loss at step %d: %f' % (i, l))\n                print(session.run(decay_rate))\n\n        prediction1 =session.run(test_prediction,feed_dict={tf_test_dataset:test_data})\n\n        class_label=tf.one_hot(tf.nn.top_k(prediction1).indices,tf.shape(prediction1)[0])\n        one_hot_label_prediction=class_label.eval()\n\n\n        for item in one_hot_label_prediction:\n            decode = tf.argmax(item, axis=1)\n            prediction.append(session.run(decode))\n\n\n\n        other_op=label.inverse_transform(prediction)\n\n        print(len(other_op))\n        print(type(other_op))\n        other_op=list(other_op)\n\n        submit = pd.DataFrame(index= only_index,data=other_op)\n        print(submit)\n        print(type(submit))\n        submit.to_csv('prediction.csv')\n        \n        put_file(credentials_1,'prediction.csv')\n        print(\"File Submitted\")\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.0", 
            "name": "python3-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "version": "3.5.2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "nbconvert_exporter": "python"
        }
    }, 
    "nbformat_minor": 1, 
    "nbformat": 4
}